{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter6_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mckang1020/deep-learning-from-scratch-practice/blob/master/Chapter6_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGOfPRvu0Ugd",
        "colab_type": "text"
      },
      "source": [
        "### 제 6장. 순환 신경망 RNN (Recurrent Neural Network)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBw0zTyPzNdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJPS6WzozY5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_hidden = 35\n",
        "lr = 0.01\n",
        "epochs = 100\n",
        "\n",
        "string = \"hello pytorch. how long can a rnn cell remember?\"\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxdmHPdK5_l2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "*   어떤 문장이 들어왔을 때 맨 앞에 시작 토큰(start token)과 맨 뒤에 끝 토큰(end token)을 붙이고, 원-핫 벡터로 변환하여 전달하는 함수 생성  \n",
        "*   반대로 원-핫 벡터를 단어로 바꿔주는 함수 생성\n",
        "\n",
        "*   문자를 그대로 쓰지않고 one-hot 벡터로 바꿔서 연산에 쓰도록 하겠습니다.\n",
        "\n",
        "Start = [0 0 0 … 1 0] <br/> \n",
        "　　a = [1 0 0 … 0 0] <br/>\n",
        "　　b =     [0 1 0 … 0 0] <br/>\n",
        "　　c =     [0 0 1 … 0 0] <br/>\n",
        "　　　... <br/>\n",
        "　end =   [0 0 0 … 0 1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY79PeyFzY8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_onehot(string):\n",
        "  start = np.zeros(shape=len(char_list), dtype=int)\n",
        "  end = np.zeros(shape=len(char_list), dtype=int)\n",
        "  start[-2] = 1\n",
        "  end[-1] = 1\n",
        "  for i in string:\n",
        "    idx = char_list.index(i)  \n",
        "    zero = np.zeros(shape=n_letters, dtype=int)  # 0으로 구성된 배열을 생성\n",
        "    zero[idx] = 1                                # 해당 string에 대응하는 0에 1을 할당 (본인 자리만 1)\n",
        "    start = np.vstack([start, zero])  # 수직으로 (vertical) 쌓아 합치는 함수, start 아래에 문자열 관련 벡터를 쌓음\n",
        "  output = np.vstack([start, end])  # 문자열이 다 끝나면 쌓아온 start에 end를 추가\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t79XteuAzZHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot_to_word(onehot_1):\n",
        "  onehot = torch.Tensor.numpy(onehot_1)  # 텐서 -> 넘파이 배열로 변환\n",
        "  return char_list[onehot.argmax()]  # 원소 중 가장 큰 값(여기선 1) 인덱스를 뽑고 -> char_list에서 찾음"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evl8g2Kl8pAU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   class 형태의 모델은 항상 `nn.Module` 을 상속받아야 하며, `super(모델명, self).__init__()` 을 실행시키는 코드가 필요\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PHMfk9IzZJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 은닉층이 하나인 RNN\n",
        "\n",
        "class RNN(nn.Module):  # ◆ 상속 개념?\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "\n",
        "    self.i2h = nn.Linear(input_size, hidden_size)  # affine operation 층\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(hidden_size, output_size)\n",
        "    self.act_fn = nn.Tanh()\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    hidden = self.act_fn(self.i2h(input)+self.h2h(hidden))\n",
        "    output = self.i2o(hidden)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self): \n",
        "    return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "rnn = RNN(n_letters, n_hidden, n_letters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGpHN_oyNv_E",
        "colab_type": "text"
      },
      "source": [
        "# 1. 상속 (Inheritance)\n",
        "\n",
        "출처: [파이썬-기본을 갈고 닦자 / 상속](https://wikidocs.net/16073)\n",
        "\n",
        "* 클래스에서 상속이란, 물려주는 클래스(*parent class, super class*)의 내용(*속성과 메소드*)을 물려받는 클래스(*child class, sub class*)가 가지게 되는 것\n",
        "\n",
        "```\n",
        "class 부모클래스:\n",
        "  ...내용...\n",
        "\n",
        "class 자식클래스(부모클래스):\n",
        "  ...내용...\n",
        "```\n",
        "\n",
        "# 2. 메소드 오버라이딩 (Method overriding)\n",
        "\n",
        "2-1. 일반적인 메소드 오버라이딩\n",
        "* 메소드 오버라이딩은 부모 클래스의 메소드를 자식 클래스에서 재정의하는 것\n",
        "\n",
        "2-2 부모 메소드'도' 호출하는 오버라이딩\n",
        "* 부모클래스의 메소드도 수행하고, 자식클래스의 메소드의 내용도 함께 출력하고 싶은 경우\n",
        "* 그럴때 `super()` 라는 키워드를 사용하면 자식클래스 내에서 코드에서도 부모클래스를 호출할 수 있음\n",
        "\n",
        "```\n",
        "class Country:\n",
        "    \"\"\"Super Class\"\"\"\n",
        "\n",
        "    name = '국가명'\n",
        "    population = '인구'\n",
        "    capital = '수도'\n",
        "\n",
        "    def show(self):\n",
        "        print('국가 클래스의 메소드입니다.')\n",
        "\n",
        "class Korea(Country):\n",
        "    \"\"\"Sub Class\"\"\"\n",
        "\n",
        "    def __init__(self, name,population, capital):\n",
        "        self.name = name\n",
        "        self.population = population\n",
        "        self.capital = capital\n",
        "    \n",
        "    def show(self):\n",
        "        super().show()\n",
        "        print(\n",
        "            \"\"\"\n",
        "            국가의 이름은 {} 입니다.\n",
        "            국가의 인구는 {} 입니다.\n",
        "            국가의 수도는 {} 입니다.\n",
        "            \"\"\".format(self.name, self.population, self.capital)\n",
        "        )\n",
        "```\n",
        "\n",
        "```\n",
        ">>> from inheritance import *\n",
        ">>> a = Korea('대한민국', 50000000, '서울')\n",
        ">>> a.show()\n",
        "국가 클래스의 메소드입니다.\n",
        "\n",
        "            국가의 이름은 대한민국 입니다.\n",
        "            국가의 인구는 50000000 입니다.\n",
        "            국가의 수도는 서울 입니다.\n",
        "\n",
        ">>> \n",
        "```\n",
        "\n",
        "# 3. 다중상속 (Mutiple Inheritance)\n",
        "\n",
        "* 여러 기반 클래스로부터 상속을 받아서 파생 클래스를 만드는 방법\n",
        "* `C#` 또는 `Java` 는 다중상속이 불가능한 언어\n",
        "* `Python` 과 `C++` 에서는 가능\n",
        "\n",
        "```\n",
        "class 부모클래스1:\n",
        "    ...내용...\n",
        "\n",
        "class 부모클래스2:\n",
        "    ...내용...    \n",
        "\n",
        "class 자식클래스(부모클래스1, 부모클래스2):\n",
        "    ...내용...\n",
        "```\n",
        "* 다중상속을 이용하여 다음과 같은 상속도 가능\n",
        "* `class D` 에서 `class A` 메소드 사용 가능 \n",
        "* 그러나 B 와 C 중 경우에 따라 호출하는 것이 달라진다면 문제가 발생할 수 있어 바람직하지 않음\n",
        "\n",
        "```\n",
        "class A:\n",
        "    def greeting(self):\n",
        "        print('안녕하세요. A입니다.')  \n",
        "\n",
        "class B(A):\n",
        "    def greeting(self):\n",
        "        print('안녕하세요. B입니다.')\n",
        "\n",
        "class C(A):\n",
        "    def greeting(self):\n",
        "        print('안녕하세요. C입니다.')\n",
        "\n",
        "class D(B, C):\n",
        "    pass\n",
        " \n",
        ">>> x = D()\n",
        ">>> x.greeting()\n",
        "안녕하세요. B입니다.\n",
        "\n",
        "```\n",
        "\n",
        "![다이아몬드 상속](https://drive.google.com/uc?id=1MUarwisAEHy1fwD3xoqMlSgtc19pl2sk)\n",
        "\n",
        "# 4. 메서드 탐색 순서 확인하기\n",
        "* 많은 프로그래밍 언어들이 다이아몬드 상속에 대한 해결책을 제시\n",
        "* `Python` 에서는 메서드 탐색 순서(Method Resolution Order, MRO) 를 따름\n",
        "* `클래스.mro()` 로 사용\n",
        "\n",
        "```\n",
        ">>> D.mro()\n",
        "[<class '__main__.D'>, <class '__main__.B'>, <class '__main__.C'>, <class '__main__.A'>, <class 'object'>]\n",
        "```\n",
        "* D 로 인스턴스를 만들고 greeting을 호출하면 B의 greeting 이 호출됨 (D에는 greeting 메서드가 없으므로)\n",
        "* `class D(B, C)` 에서 왼쪽에서 오른쪽 순서로 메서드를 찾음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elHaJ8FSBAdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yOj_2vv2gTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.ones(1)\n",
        "torch.ones(1,1)\n",
        "torch.ones(1,1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiDHRLaNWEJL",
        "colab_type": "code",
        "outputId": "8c8002f0-4979-4822-c75d-f2e80e0fd273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())  # ◆ `FloatTensor`는 함수? 데이터형이 들어가는 것 아님?\n",
        "\n",
        "for i in range(epochs):\n",
        "  rnn.zero_grad()  # ◆ 아래 설명 참고\n",
        "  total_loss = 0\n",
        "  hidden = rnn.init_hidden()  # 초기화\n",
        "\n",
        "  for j in range(one_hot.size()[0]-1):  # 입력은 앞에 글자 pythorch 에서 p y t o r c h\n",
        "    input_ = one_hot[j:j+1,:]  # ◆ one_hot[j+1,:] 이렇게 하면 안되나? 이렇게 하는 이유? -> 매트릭스 화\n",
        "    target = one_hot[j+1]\n",
        "\n",
        "    output, hidden = rnn.forward(input_, hidden)\n",
        "    loss = loss_func(output.view(-1), target.view(-1))\n",
        "    total_loss += loss\n",
        "    input_ = output  # ◆ 변수명에 '_'는 왜 붙이는 것? in-place? 함수에만 붙이는 것 아니였나? 명시적이기 위해?\n",
        "\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    print(total_loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.6027, grad_fn=<AddBackward0>)\n",
            "tensor(1.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.6796, grad_fn=<AddBackward0>)\n",
            "tensor(0.4723, grad_fn=<AddBackward0>)\n",
            "tensor(0.3244, grad_fn=<AddBackward0>)\n",
            "tensor(0.2246, grad_fn=<AddBackward0>)\n",
            "tensor(0.2080, grad_fn=<AddBackward0>)\n",
            "tensor(0.1485, grad_fn=<AddBackward0>)\n",
            "tensor(0.1207, grad_fn=<AddBackward0>)\n",
            "tensor(0.1006, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Vb8XeIcmQl",
        "colab_type": "text"
      },
      "source": [
        "* [`rnn.zero_grad()`](https://discuss.pytorch.org/t/zero-grad-optimizer-or-net/1887) 에 대한 설명\n",
        "* 파이토치에서는 backward를 할때 gradients를 축적하기 때문에 매번 0으로 바꿔주어야 한다. \n",
        "![결국 같다](https://drive.google.com/uc?id=1L75xuuWMGZhww68h9CJvbcLsFmb8-cda)\n",
        "\n",
        "![opt.zero_grad() vs model.zero_grad()](https://drive.google.com/uc?id=1-cWzK2PNNuE-mC_Zy_xGRixLs6cAVszA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_0B9I2UWEOd",
        "colab_type": "code",
        "outputId": "52272f33-166d-4e5b-8373-765318f967ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "start = torch.zeros(1,len(char_list))\n",
        "start[:,-2] = 1\n",
        "\n",
        "with torch.no_grad():   # ◆ 아래 설명 참고\n",
        "  hidden = rnn.init_hidden()\n",
        "  input_ = start        # 첫 입력값은 start token\n",
        "  output_string = \"\"    # 결과로 나오는 문자들을 계속 붙여준다\n",
        "  for i in range(len(string)):\n",
        "    output, hidden = rnn.forward(input_, hidden)\n",
        "    output_string += onehot_to_word(output.data)  # x가 variable 일 때, x.data 는 그 값을 갖는 tensor\n",
        "    input_ = output\n",
        "\n",
        "print(output_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "helloreytoec r n r ron an n r r n  n  c c   n  n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eaYXbwcwlle",
        "colab_type": "text"
      },
      "source": [
        "# 1. try - finally 블록\n",
        "* 파일의 작업 흐름은 `open()` 내장함수로 파일을 열고, 파일 객체를 통해 파일을 작업하고, 파일 객체의 `close()` 함수로 파일을 닫아야 함\n",
        "* 파일을 닫지 않으면 데이터가 소실될 수 있음\n",
        "* try - finally 절로 묶는 이유는 예외가 발생을 해도 finally 절을 통해 반드시 파일을 close 시킬 수 있기 때문\n",
        "\n",
        "```\n",
        "try:\n",
        "  변수 = open(파일경로, 옵션)\n",
        "  ... 파일 조작 ...\n",
        "finally:\n",
        "  변수.close()\n",
        "``` \n",
        "<br>\n",
        "\n",
        "# 2. with 블록\n",
        "* 같은 작업이지만, with 블록을 통해 명시적으로 `close()` 메소드를 호출하지 않고도 파일을 닫을 수 있음\n",
        "* with 블록이 자동으로 블록을 종료할 때 `__exit()__` 메소드를 호출하여 파일을 close 시키게 됨\n",
        "\n",
        "<br>\n",
        "\n",
        "# 3. 따라서 `with torch.no_grad()` 의 의미는?\n",
        "출처: [tutorials.pytorch.kr](https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html)\n",
        "\n",
        "> `torch.Tensor` 클래스에서 `.requires_grad` 속성을 `True` 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작한다. <br><br> 계산이 완료된 후 `.backward()` 를 호출하여 모든 변화도(gradients)를 자동으로 계산할 수 있다. 이 tensor의 변화도는 `.grad` 속성에 누적된다.<br><br> tensor가 기록하는 것을 중단하게 하려면, `.detach()` 를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있다.\n",
        "\n",
        "* 기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하지 위해, 코드 블록을 `with torch.no_grad():` 로 감쌀 수 있다. 이는 특히 변화도(gradient)는 필요없지만, `requires_grad = True` 가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용하다.\n",
        "<br><br>\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0ms06Ms_duO",
        "colab_type": "text"
      },
      "source": [
        "# 코드 차이점 비교 분석 (책 vs. colab)\n",
        "\n",
        "![RNN 계층 layer 비교](https://drive.google.com/uc?id=1KXGE9FcuVY5a5p8au47iSiY-Q2MbEY1K)\n",
        "\n",
        "* 왼쪽 코드를 (1)번, 오른쪽 코드를 (2)번이라고 하자\n",
        "* 1번에서 `i2h`, `h2h` 계층은 RNN 아키텍쳐 상 이해가 가지만, `i2o` 에는 동의하지 못하겠음\n",
        "* hidden layer에서 output layer이면 `h2o` 이 더 자연스러운듯 함\n",
        "<br><br>\n",
        "* 1번에서는 일반적인 RNN 계층 처럼 input값과 hidden값을 더해 `i2o` 에 넣어 output를 도출하고, return 값으로 나온 그 output값과 사용되었던 hidden값을 다시 다음 글자를 맞추는데(output를 도출) 사용\n",
        "* 그러나, 2번에서는 input값과 hidden값을 병합시켰음(물론 크기도 `nn.Linear(input_size + hidden_size)` 로 맞춰줌)\n",
        "* 이유는??? -> 시퀀스 특성을 보존하기 위해\n",
        "\n",
        "![concat layer](https://drive.google.com/uc?id=1frG3EHKg6Wvm9XfDgieTMhi9FlAnFD0p)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZAigfYuZp_M",
        "colab_type": "text"
      },
      "source": [
        "# LSTM (Long short-term memory)\n",
        "\n",
        "![LSTM](https://drive.google.com/uc?id=1VhcVvNMn8IzQ4yR-DoBpleoCAYWVuzVQ)\n",
        "\n",
        "* 기존의 순환 신경망 모델에 장기기억<sup>long-term memory</sup>을 담당하는 부분을 추가한 것\n",
        "* 기존에 있던 은닉 상태<sup>hidden state</sup>에 셀 상태<sup>cell state</sup>를 추가함\n",
        "* 망각 게이트와 입력 게이트를 통해 셀 상태의 값을 얼마나 잊어버릴지 또는 기억할지를 조절\n",
        "* 다시 말해, 현재 시점의 새로운 입력값과 직전 시점의 은닉 상태의 값의 조합으로 기존의 셀 상태의 정보를 얼마큼 전달할지도 정하고 어떤 정보를 얼마만큼의 비중을 더할지도 정하는 것\n",
        "\n",
        "★ tanh 함수 쓰는 이유 ReLU 안되는 이유?\n",
        "[설명](https://stackoverflow.com/questions/40761185/what-is-the-intuition-of-using-tanh-in-lstm)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThYRea7_cwxY",
        "colab_type": "text"
      },
      "source": [
        "### 1) 예시\n",
        "\n",
        "파라미터 | 하는 일\n",
        "--- | ---\n",
        "input_size | 입력의 특성 개수\n",
        "hidden_size | hidden state의 특성 개수\n",
        "num_layers | LSTM을 몇층으로 쌓을것인가 여부\n",
        "bias | 편차의 사용 여부\n",
        "batch_first | 사용하면 입력과 출력의 형태가 [batch, seq, feature]\n",
        "dropout | 드롭아웃 사용여부\n",
        "bidirectional | 양방향 LSTM 사용여부 (기본이 False)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xCQ3em4Asuv",
        "colab_type": "code",
        "outputId": "456fb379-5900-487a-f709-808c1ee639f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "rnn = nn.LSTM(input_size=3, hidden_size=5, num_layers=2)\n",
        "\n",
        "# ◆ 위에서와 같은 질문 왜 input_ 에서 _를 붙이는 거지?\n",
        "# 기본적으로 입력의 형태는 (seq_len, batch, input_size)\n",
        "input_ = torch.randn(5, 3, 3)  \n",
        "\n",
        "# '히든'과 '셀'의 형태는 (num_layer * num_directions, batch, hidden_size) \n",
        "# ◆ num_directions는 bidirectional 일때 2, 그렇다면 어차피 그 값은 1 또는 2 ?\n",
        "h0 = torch.randn(2, 3, 5)\n",
        "c0 = torch.randn(2, 3, 5)\n",
        "\n",
        "# LSTM에 입력을 전달할때는 동일하게 input, (h_0, c_0) 처럼 상태를 튜플로 묶어서 전달\n",
        "output, (hidden_state, cell_state) = rnn(input_, (h0, c0)) \n",
        "\n",
        "print(output.size(),hidden_state.size(),cell_state.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 5]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nny8QZUPW4Ck",
        "colab_type": "text"
      },
      "source": [
        "### 2) 하드 코딩\n",
        "* batch_first를 사용, 즉 입력과 출력의 형태가 `[batch, seq, feature]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89q5A1Z_dwJR",
        "colab_type": "code",
        "outputId": "bb662f0e-d3aa-4717-a1c2-b272a98b554c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "rnn = nn.LSTM(3, 5, 2, batch_first=True)\n",
        "\n",
        "# batch_first를 사용하면 입력의 형태는 (batch, seq, feature)\n",
        "input_ = torch.randn(3, 5, 3)\n",
        "\n",
        "# '히든'과 '셀'의 형태는 동일하게 (num_layers * num_directions, batch, hidden_size)\n",
        "h0 = torch.randn(2, 3, 5)\n",
        "c0 = torch.randn(2, 3, 5)\n",
        "\n",
        "# LSTM에 입력을 전달할때는 동일하게 input, (h_0, c_0) 처럼 상태를 튜플로 묶어서 전달\n",
        "output, (hidden_state, cell_state) = rnn(input_, (h0, c0))\n",
        "\n",
        "print(input_.size(),h0.size(),c0.size())\n",
        "print(output.size(),hidden_state.size(),cell_state.size())  # ◆★ 인풋의 feature는 3인데, 아웃풋의 feature는 5 ..?"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5, 3]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n",
            "torch.Size([3, 5, 5]) torch.Size([2, 3, 5]) torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ntKyxahYARY",
        "colab_type": "text"
      },
      "source": [
        "### 3) 하이퍼 파라미터로 표현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_pjP2t2dwM6",
        "colab_type": "code",
        "outputId": "59876d4a-e032-490b-b597-d972b1b545a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "input_size = 7\n",
        "batch_size = 4\n",
        "hidden_size = 3\n",
        "seq_len = 2\n",
        "\n",
        "rnn = nn.LSTM(input_size, hidden_size, seq_len, batch_first=True)\n",
        "\n",
        "input_ = torch.randn(batch_size, hidden_size, input_size)\n",
        "h0 = torch.randn(seq_len, batch_size, hidden_size)\n",
        "c0 = torch.randn(seq_len, batch_size, hidden_size)\n",
        "\n",
        "output, (hidden_state, cell_state) = rnn(input_, (h0, c0))\n",
        "\n",
        "print(input_.size(),h0.size(),c0.size())\n",
        "print(output.size(),hidden_state.size(),cell_state.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 7]) torch.Size([2, 4, 3]) torch.Size([2, 4, 3])\n",
            "torch.Size([4, 3, 3]) torch.Size([2, 4, 3]) torch.Size([2, 4, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQVx3e6cYHxb",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5V-8hEeYVqc",
        "colab_type": "text"
      },
      "source": [
        "### 같은 task를 `RNN`에서 `LSTM`으로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_CRTis5dwV_",
        "colab_type": "code",
        "outputId": "400614d3-045d-41fa-f3fd-22d192b47aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "string = \"hello pytorch. how long can a rnn cell remember? show me your limit!\"\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
        "char_list = [i for i in chars]\n",
        "char_len = len(char_list)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "# 문자열을 단어 하나씩 잘러서 사용하는걸로 구현하여 batch_size 1로 고정\n",
        "# batch_size가 1보다 큰 경우는 다음 실습코드\n",
        "batch_size = 1\n",
        "\n",
        "# seq_len는 바꿔도 학습은 되지만 테스트시 편의성을 위해 1로 설정\n",
        "# ◆ sequence length 를 의미? \n",
        "seq_len = 1\n",
        "\n",
        "# num_layers는 입력 형식에만 맞게 형태를 바꿔주면 됩니다.\n",
        "num_layers = 3\n",
        "input_size = char_len\n",
        "hidden_size = 35 \n",
        "lr = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "print(one_hot.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([70, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuGXMww0Yzk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN with 1 hidden layer\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers)\n",
        "        \n",
        "    def forward(self,input_,hidden,cell):\n",
        "        output,(hidden,cell) = self.lstm(input_,(hidden,cell))\n",
        "        return output,hidden,cell\n",
        "    \n",
        "    def init_hidden_cell(self):\n",
        "        hidden = torch.zeros(num_layers,batch_size,hidden_size)\n",
        "        cell = torch.zeros(num_layers,batch_size,hidden_size)\n",
        "        return hidden,cell\n",
        "    \n",
        "rnn = RNN(input_size,hidden_size, num_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0HVeqV7Yzqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss function & Optimizer\n",
        "\n",
        "loss_func = nn.MSELoss()  # ★ 크로스 엔트로피 안쓰는 이유?\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUWeUaFUYztW",
        "colab_type": "code",
        "outputId": "72052283-7f14-4727-a51d-51ae7380fed2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "j=0\n",
        "input_data = one_hot[j:j+seq_len].view(seq_len, batch_size, input_size)\n",
        "print(input_data.size())\n",
        "\n",
        "hidden,cell = rnn.init_hidden_cell()\n",
        "print(hidden.size(),cell.size())\n",
        "\n",
        "output, hidden,cell = rnn(input_data,hidden,cell)\n",
        "print(output.size(),hidden.size(),cell.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 35])\n",
            "torch.Size([3, 1, 35]) torch.Size([3, 1, 35])\n",
            "torch.Size([1, 1, 35]) torch.Size([3, 1, 35]) torch.Size([3, 1, 35])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftYlMqCCY9RI",
        "colab_type": "code",
        "outputId": "30747417-7d58-4cb6-b6db-62e755fbdb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "unroll_len = one_hot.size()[0]//seq_len -1\n",
        "for i in range(num_epochs):\n",
        "    hidden,cell = rnn.init_hidden_cell()\n",
        "    \n",
        "    loss = 0\n",
        "    for j in range(unroll_len):\n",
        "        input_data = one_hot[j:j+seq_len].view(seq_len,batch_size,input_size) \n",
        "        label = one_hot[j+1:j+seq_len+1].view(seq_len,batch_size,input_size)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, hidden, cell = rnn(input_data,hidden,cell)\n",
        "        loss += loss_func(output.view(1,-1),label.view(1,-1))\n",
        "        \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i%10 ==0:\n",
        "        print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.3196, grad_fn=<AddBackward0>)\n",
            "tensor(1.8022, grad_fn=<AddBackward0>)\n",
            "tensor(1.6776, grad_fn=<AddBackward0>)\n",
            "tensor(1.5151, grad_fn=<AddBackward0>)\n",
            "tensor(1.2524, grad_fn=<AddBackward0>)\n",
            "tensor(0.8537, grad_fn=<AddBackward0>)\n",
            "tensor(0.4916, grad_fn=<AddBackward0>)\n",
            "tensor(0.2358, grad_fn=<AddBackward0>)\n",
            "tensor(0.1195, grad_fn=<AddBackward0>)\n",
            "tensor(0.0725, grad_fn=<AddBackward0>)\n",
            "tensor(0.0519, grad_fn=<AddBackward0>)\n",
            "tensor(0.0347, grad_fn=<AddBackward0>)\n",
            "tensor(0.0258, grad_fn=<AddBackward0>)\n",
            "tensor(0.0199, grad_fn=<AddBackward0>)\n",
            "tensor(0.0161, grad_fn=<AddBackward0>)\n",
            "tensor(0.0138, grad_fn=<AddBackward0>)\n",
            "tensor(0.0124, grad_fn=<AddBackward0>)\n",
            "tensor(0.0114, grad_fn=<AddBackward0>)\n",
            "tensor(0.0108, grad_fn=<AddBackward0>)\n",
            "tensor(0.0104, grad_fn=<AddBackward0>)\n",
            "tensor(0.0099, grad_fn=<AddBackward0>)\n",
            "tensor(0.0094, grad_fn=<AddBackward0>)\n",
            "tensor(0.0091, grad_fn=<AddBackward0>)\n",
            "tensor(0.0093, grad_fn=<AddBackward0>)\n",
            "tensor(0.0088, grad_fn=<AddBackward0>)\n",
            "tensor(0.0085, grad_fn=<AddBackward0>)\n",
            "tensor(0.0082, grad_fn=<AddBackward0>)\n",
            "tensor(0.0080, grad_fn=<AddBackward0>)\n",
            "tensor(0.0076, grad_fn=<AddBackward0>)\n",
            "tensor(0.0068, grad_fn=<AddBackward0>)\n",
            "tensor(0.0065, grad_fn=<AddBackward0>)\n",
            "tensor(0.0063, grad_fn=<AddBackward0>)\n",
            "tensor(0.0062, grad_fn=<AddBackward0>)\n",
            "tensor(0.0061, grad_fn=<AddBackward0>)\n",
            "tensor(0.0060, grad_fn=<AddBackward0>)\n",
            "tensor(0.0059, grad_fn=<AddBackward0>)\n",
            "tensor(0.0064, grad_fn=<AddBackward0>)\n",
            "tensor(0.0062, grad_fn=<AddBackward0>)\n",
            "tensor(0.0058, grad_fn=<AddBackward0>)\n",
            "tensor(0.0056, grad_fn=<AddBackward0>)\n",
            "tensor(0.0055, grad_fn=<AddBackward0>)\n",
            "tensor(0.0054, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0046, grad_fn=<AddBackward0>)\n",
            "tensor(0.0039, grad_fn=<AddBackward0>)\n",
            "tensor(0.0036, grad_fn=<AddBackward0>)\n",
            "tensor(0.0033, grad_fn=<AddBackward0>)\n",
            "tensor(0.0031, grad_fn=<AddBackward0>)\n",
            "tensor(0.0029, grad_fn=<AddBackward0>)\n",
            "tensor(0.0028, grad_fn=<AddBackward0>)\n",
            "tensor(0.0028, grad_fn=<AddBackward0>)\n",
            "tensor(0.0027, grad_fn=<AddBackward0>)\n",
            "tensor(0.0026, grad_fn=<AddBackward0>)\n",
            "tensor(0.0026, grad_fn=<AddBackward0>)\n",
            "tensor(0.0029, grad_fn=<AddBackward0>)\n",
            "tensor(0.0026, grad_fn=<AddBackward0>)\n",
            "tensor(0.0025, grad_fn=<AddBackward0>)\n",
            "tensor(0.0025, grad_fn=<AddBackward0>)\n",
            "tensor(0.0024, grad_fn=<AddBackward0>)\n",
            "tensor(0.0024, grad_fn=<AddBackward0>)\n",
            "tensor(0.0024, grad_fn=<AddBackward0>)\n",
            "tensor(0.0024, grad_fn=<AddBackward0>)\n",
            "tensor(0.0023, grad_fn=<AddBackward0>)\n",
            "tensor(0.0023, grad_fn=<AddBackward0>)\n",
            "tensor(0.0023, grad_fn=<AddBackward0>)\n",
            "tensor(0.0023, grad_fn=<AddBackward0>)\n",
            "tensor(0.0023, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0021, grad_fn=<AddBackward0>)\n",
            "tensor(0.0021, grad_fn=<AddBackward0>)\n",
            "tensor(0.0021, grad_fn=<AddBackward0>)\n",
            "tensor(0.0021, grad_fn=<AddBackward0>)\n",
            "tensor(0.0021, grad_fn=<AddBackward0>)\n",
            "tensor(0.0022, grad_fn=<AddBackward0>)\n",
            "tensor(0.0021, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0020, grad_fn=<AddBackward0>)\n",
            "tensor(0.0019, grad_fn=<AddBackward0>)\n",
            "tensor(0.0019, grad_fn=<AddBackward0>)\n",
            "tensor(0.0019, grad_fn=<AddBackward0>)\n",
            "tensor(0.0019, grad_fn=<AddBackward0>)\n",
            "tensor(0.0019, grad_fn=<AddBackward0>)\n",
            "tensor(0.0019, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrAqXF6RY9tx",
        "colab_type": "code",
        "outputId": "3f346800-0fea-4d22-d15d-fd865ca16265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "hidden,cell = rnn.init_hidden_cell()\n",
        "\n",
        "for j in range(unroll_len-1):\n",
        "    input_data = one_hot[j:j+1].view(1,batch_size,hidden_size) \n",
        "    label = one_hot[j+1:j+1+1].view(1,batch_size,hidden_size) \n",
        "    \n",
        "    output, hidden, cell = rnn(input_data,hidden,cell)\n",
        "    print(onehot_to_word(output.data),end=\"\") "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello pytorch. how long can a rnn cell remember? show me your limit!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQGL2DI8FjdT",
        "colab_type": "text"
      },
      "source": [
        " ★ 1. RNN 크기 관련 그림\n",
        "    2. RNN에서 MSE Loss 를 쓰는 이유"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niNr00kQaFxT",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "### LSTM: batch size를 3으로 증가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsCLMTRUaE3m",
        "colab_type": "code",
        "outputId": "7ca270f6-268f-40ae-b0e3-f3b4b0778a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "string = \"hello pytorch. how long can a rnn cell remember? show me your limit!\"\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz ?!.,:;01\"\n",
        "char_list = [i for i in chars]\n",
        "char_len = len(char_list)\n",
        "\n",
        "def string_to_onehot(string):\n",
        "    start = np.zeros(shape=char_len ,dtype=int)\n",
        "    end = np.zeros(shape=char_len ,dtype=int)\n",
        "    start[-2] = 1\n",
        "    end[-1] = 1\n",
        "    for i in string:\n",
        "        idx = char_list.index(i)\n",
        "        zero = np.zeros(shape=char_len ,dtype=int)\n",
        "        zero[idx]=1\n",
        "        start = np.vstack([start,zero])\n",
        "    output = np.vstack([start,end])\n",
        "    return output\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "# 이번코드는 배치사이즈가 1보다 큰 경우\n",
        "batch_size = 5\n",
        "\n",
        "# seq_len는 바꿔도 학습은 되지만 테스트시 편의성을 위해 1로 설정\n",
        "seq_len = 1\n",
        "\n",
        "# num_layers는 자유롭게 바꿀 수 있음\n",
        "num_layers = 3\n",
        "input_size = char_len\n",
        "hidden_size = 35 \n",
        "lr = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        \n",
        "    def forward(self,input,hidden,cell):\n",
        "        output,(hidden,cell) = self.lstm(input,(hidden,cell))\n",
        "        return output,hidden,cell\n",
        "    \n",
        "    def init_hidden_cell(self):\n",
        "        hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "        cell = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "        return hidden,cell\n",
        "    \n",
        "rnn = RNN(input_size,hidden_size, num_layers)\n",
        "\n",
        "# Loss function & Optimizer\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
        "\n",
        "j=0\n",
        "input_data = one_hot[j:j+batch_size].view(batch_size,seq_len,input_size)\n",
        "print(input_data.size())\n",
        "\n",
        "hidden,cell = rnn.init_hidden_cell()\n",
        "print(hidden.size(),cell.size())\n",
        "\n",
        "output,hidden,cell = rnn(input_data,hidden,cell)\n",
        "print(output.size(),hidden.size(),cell.size())\n",
        "\n",
        "unroll_len = one_hot.size()[0]//seq_len -1\n",
        "for i in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    hidden,cell = rnn.init_hidden_cell()\n",
        "    \n",
        "    loss = 0\n",
        "    for j in range(unroll_len-batch_size+1):\n",
        "        \n",
        "        # batch size에 맞게 one-hot 벡터를 스택\n",
        "        # 예를 들어 batch size가 3이면 pytorch에서 pyt를 one-hot 벡터로 바꿔서 쌓고 목표값으로 yto를 one-hot 벡터로 바꿔서 쌓는 과정\n",
        "        input_data = torch.stack([one_hot[j+k:j+k+seq_len] for k in range(batch_size)],dim=0)\n",
        "        label = torch.stack([one_hot[j+k+1:j+k+seq_len+1] for k in range(batch_size)],dim=0)\n",
        "        \n",
        "        input_data = input_data\n",
        "        label = label\n",
        "        \n",
        "        output, hidden, cell = rnn(input_data,hidden,cell)\n",
        "        loss += loss_func(output.view(1,-1),label.view(1,-1))\n",
        "        \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 1, 35])\n",
            "torch.Size([3, 3, 35]) torch.Size([3, 3, 35])\n",
            "torch.Size([3, 1, 35]) torch.Size([3, 3, 35]) torch.Size([3, 3, 35])\n",
            "tensor(2.2764, grad_fn=<AddBackward0>)\n",
            "tensor(1.7482, grad_fn=<AddBackward0>)\n",
            "tensor(1.6716, grad_fn=<AddBackward0>)\n",
            "tensor(1.5531, grad_fn=<AddBackward0>)\n",
            "tensor(1.3611, grad_fn=<AddBackward0>)\n",
            "tensor(1.0665, grad_fn=<AddBackward0>)\n",
            "tensor(0.6247, grad_fn=<AddBackward0>)\n",
            "tensor(0.3634, grad_fn=<AddBackward0>)\n",
            "tensor(0.1737, grad_fn=<AddBackward0>)\n",
            "tensor(0.0981, grad_fn=<AddBackward0>)\n",
            "tensor(0.0633, grad_fn=<AddBackward0>)\n",
            "tensor(0.0448, grad_fn=<AddBackward0>)\n",
            "tensor(0.0344, grad_fn=<AddBackward0>)\n",
            "tensor(0.0277, grad_fn=<AddBackward0>)\n",
            "tensor(0.0237, grad_fn=<AddBackward0>)\n",
            "tensor(0.0210, grad_fn=<AddBackward0>)\n",
            "tensor(0.0191, grad_fn=<AddBackward0>)\n",
            "tensor(0.0177, grad_fn=<AddBackward0>)\n",
            "tensor(0.0190, grad_fn=<AddBackward0>)\n",
            "tensor(0.0160, grad_fn=<AddBackward0>)\n",
            "tensor(0.0146, grad_fn=<AddBackward0>)\n",
            "tensor(0.0138, grad_fn=<AddBackward0>)\n",
            "tensor(0.0132, grad_fn=<AddBackward0>)\n",
            "tensor(0.0127, grad_fn=<AddBackward0>)\n",
            "tensor(0.0122, grad_fn=<AddBackward0>)\n",
            "tensor(0.0115, grad_fn=<AddBackward0>)\n",
            "tensor(0.0109, grad_fn=<AddBackward0>)\n",
            "tensor(0.0105, grad_fn=<AddBackward0>)\n",
            "tensor(0.0103, grad_fn=<AddBackward0>)\n",
            "tensor(0.0100, grad_fn=<AddBackward0>)\n",
            "tensor(0.0099, grad_fn=<AddBackward0>)\n",
            "tensor(0.0097, grad_fn=<AddBackward0>)\n",
            "tensor(0.0095, grad_fn=<AddBackward0>)\n",
            "tensor(0.0117, grad_fn=<AddBackward0>)\n",
            "tensor(0.0100, grad_fn=<AddBackward0>)\n",
            "tensor(0.0093, grad_fn=<AddBackward0>)\n",
            "tensor(0.0092, grad_fn=<AddBackward0>)\n",
            "tensor(0.0090, grad_fn=<AddBackward0>)\n",
            "tensor(0.0089, grad_fn=<AddBackward0>)\n",
            "tensor(0.0088, grad_fn=<AddBackward0>)\n",
            "tensor(0.0087, grad_fn=<AddBackward0>)\n",
            "tensor(0.0086, grad_fn=<AddBackward0>)\n",
            "tensor(0.0084, grad_fn=<AddBackward0>)\n",
            "tensor(0.0079, grad_fn=<AddBackward0>)\n",
            "tensor(0.0074, grad_fn=<AddBackward0>)\n",
            "tensor(0.0073, grad_fn=<AddBackward0>)\n",
            "tensor(0.0072, grad_fn=<AddBackward0>)\n",
            "tensor(0.0066, grad_fn=<AddBackward0>)\n",
            "tensor(0.0060, grad_fn=<AddBackward0>)\n",
            "tensor(0.0058, grad_fn=<AddBackward0>)\n",
            "tensor(0.0058, grad_fn=<AddBackward0>)\n",
            "tensor(0.0058, grad_fn=<AddBackward0>)\n",
            "tensor(0.0057, grad_fn=<AddBackward0>)\n",
            "tensor(0.0056, grad_fn=<AddBackward0>)\n",
            "tensor(0.0055, grad_fn=<AddBackward0>)\n",
            "tensor(0.0055, grad_fn=<AddBackward0>)\n",
            "tensor(0.0054, grad_fn=<AddBackward0>)\n",
            "tensor(0.0054, grad_fn=<AddBackward0>)\n",
            "tensor(0.0054, grad_fn=<AddBackward0>)\n",
            "tensor(0.0054, grad_fn=<AddBackward0>)\n",
            "tensor(0.0053, grad_fn=<AddBackward0>)\n",
            "tensor(0.0053, grad_fn=<AddBackward0>)\n",
            "tensor(0.0053, grad_fn=<AddBackward0>)\n",
            "tensor(0.0054, grad_fn=<AddBackward0>)\n",
            "tensor(0.0053, grad_fn=<AddBackward0>)\n",
            "tensor(0.0053, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0052, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0051, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0049, grad_fn=<AddBackward0>)\n",
            "tensor(0.0049, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0050, grad_fn=<AddBackward0>)\n",
            "tensor(0.0049, grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxyXKpYaY9wb",
        "colab_type": "code",
        "outputId": "92f3f1e7-5b7b-41f2-821d-f67f45a83bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "hidden,cell = rnn.init_hidden_cell()\n",
        "\n",
        "for j in range(unroll_len-batch_size+1):\n",
        "    input_data = torch.stack([one_hot[j+k:j+k+seq_len] for k in range(batch_size)],dim=0)\n",
        "    label = torch.stack([one_hot[j+k+1:j+k+seq_len+1] for k in range(batch_size)],dim=0)\n",
        "\n",
        "    input_data = input_data\n",
        "    label = label\n",
        "    \n",
        "    output, hidden, cell = rnn(input_data,hidden,cell)\n",
        "    for k in range(batch_size):\n",
        "        print(onehot_to_word(output[k].data),end=\"\")\n",
        "        if j < unroll_len-batch_size:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello pytorch. how long can a rnn cell remember? show me your limit!1"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzOdVRjbfg8Q",
        "colab_type": "text"
      },
      "source": [
        "* 왜 이렇게 안나오지? 코드 점검\n",
        "![왜이렇게안나오지](https://drive.google.com/uc?id=144u-phptYRK7s8P23-k9mOr3OnP6eysZ)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEX1InvefhIV",
        "colab_type": "text"
      },
      "source": [
        "## 더 큰 데이터에 학습시키기 (feat.셰익스피어)\n",
        "\n",
        "# 1. Naive RNN\n",
        "* 셰익스피어 문체를 모방하는 순환신경망 실습\n",
        "* 임베딩<sup>Embedding</sup> 레이어 및 RNN 모델로 구성\n",
        "\n",
        "1-1. 데이터 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSNCXbfHdom8",
        "colab_type": "code",
        "outputId": "e154a1b0-dfcc-440b-f46d-d644b4e9dc01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "!rm -r data\n",
        "import os \n",
        "\n",
        "try:\n",
        "  os.mkdir(\"./data\")  # 폴더 생성 (make directory)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "!wget https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt -P ./data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n",
            "--2019-11-29 08:31:55--  https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘./data/input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-11-29 08:31:56 (11.8 MB/s) - ‘./data/input.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NryviApIerm",
        "colab_type": "text"
      },
      "source": [
        "1-2. 필요한 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUyoPlhcHaPp",
        "colab_type": "code",
        "outputId": "cc2cbdd4-cb3a-490d-d4c0-7a84da17a772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "!pip install unidecode\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "import time, math"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP1l8shXIyAe",
        "colab_type": "text"
      },
      "source": [
        "1-3. 하이퍼 파라미터 들 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izUyXj3JIuM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "\n",
        "chunk_len = 200  # 설명은 함수정의에서 함\n",
        "\n",
        "hidden_size = 100\n",
        "batch_size = 1\n",
        "num_layers = 1\n",
        "embedding_size = 70\n",
        "lr = 0.002"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvBiswtCJCEL",
        "colab_type": "text"
      },
      "source": [
        "1-4. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8eEgkZ-I-5L",
        "colab_type": "code",
        "outputId": "5741c5f9-0625-4162-d0c3-ba7891a104f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "all_characters = string.printable  # import 했던 string에서 출력가능한 문자들을 다 불러옴\n",
        "                          \n",
        "n_characters = len(all_characters)  # 출력가능한 문자들의 개수를 저장\n",
        "print(all_characters)\n",
        "print('num_chars = ', n_characters)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n",
            "num_chars =  100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z4TlrvuKSNH",
        "colab_type": "code",
        "outputId": "35589f22-de67-4cf9-9a0c-d56613ac02f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "file = unidecode.unidecode(open('./data/input.txt').read())  # text 를 읽어옴\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXs8wY5rK3tn",
        "colab_type": "text"
      },
      "source": [
        "1-5. 등장하는 함수들\n",
        "\n",
        "1) random chunk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJlwIeV-K291",
        "colab_type": "code",
        "outputId": "6fe83774-f24d-43f3-ad8f-f15ba827ed73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "# 이 함수는 텍스트 파일의 일부분을 랜덤하게 불러옴\n",
        "def random_chunk():\n",
        "    # (시작지점 < 텍스트파일 전체길이 - 불러오는 텍스트의 길이)가 되도록 시작점과 끝점 설정\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IANCA:\n",
            "If you affect him, sister, here I swear\n",
            "I'll plead for you myself, but you shall have\n",
            "him.\n",
            "\n",
            "KATHARINA:\n",
            "O then, belike, you fancy riches more:\n",
            "You will have Gremio to keep you fair.\n",
            "\n",
            "BIANCA:\n",
            "Is i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwEdSBqMvAyO",
        "colab_type": "text"
      },
      "source": [
        "2) character to tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfcp0fKdKZWU",
        "colab_type": "code",
        "outputId": "b513cf5f-4b72-4ff9-e8d9-0bb7c14d0bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 문자열을 받았을때 이를 인덱스의 배열로 바꿔주는 함수\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()  # self.long() is equivalent to self.to(torch.int64)\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return tensor\n",
        "\n",
        "print(char_tensor('ABCdef'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36, 37, 38, 13, 14, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs7mhVL2v2ZT",
        "colab_type": "text"
      },
      "source": [
        "3) chunk into input & label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNT71y3QvHJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 랜덤한 텍스트 chunk를 불러와서 이를 입력과 목표값을 바꿔주는 함수\n",
        "# ex) pytorch라는 문자열이 들어오면 입력은 pytorc / 목표값은 ytorch (output)\n",
        "def random_training_set():    \n",
        "    chunk = random_chunk()\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0rN9UEUSaDF",
        "colab_type": "text"
      },
      "source": [
        "1-6. 모델과 옵티마이져\n",
        "\n",
        "1) 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmGRyFwhShTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)  # 임베딩 함수\n",
        "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "        self.decoder = nn.Linear(self.hidden_size, self.output_size)  # 디코더\n",
        "        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        out = self.encoder(input.view(1,-1))\n",
        "        out,hidden = self.rnn(out,hidden)\n",
        "        out = self.decoder(out.view(batch_size,-1))\n",
        "        return out,hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return hidden\n",
        "    \n",
        "\n",
        "model = RNN(n_characters, embedding_size, hidden_size, n_characters, num_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIi85BrdXtBQ",
        "colab_type": "text"
      },
      "source": [
        "* 원래 실무에서는 word2vec을 미리 만들어두고 사용하지만, 여기서는 임베딩<sup>Embedding</sup> 역시 학습이 되는 방식으로 구현\n",
        "\n",
        "* `torch.nn.Embedding` 이라는 class에서 `num_embeddings`와 `embedding_dim`은 각각 사용할 문자나 단어의 수 및 임베딩할 벡터 공간의 크기를 의미\n",
        "\n",
        "> ex) a, b, c, d 의 4가지(`num_embeddings=4`) 문자를 10차원(`embedding_dim=10`)으로 표현하면, 4 x 10 벡터가 생성 되며 각각의 행이 a, b, c, d 를 의미\n",
        "\n",
        "![embedding포함 RNN](https://drive.google.com/uc?id=1XtOhAVZjx6mH_EVhPlLisFqrQK7I21Ey)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkVQmPPsTd8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(input_size=n_characters, \n",
        "            embedding_size=embedding_size,\n",
        "            hidden_size=hidden_size, \n",
        "            output_size=n_characters, \n",
        "            num_layers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GylM3tW0eksH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "88f23c43-c457-414e-e586-5fabaa7bb02d"
      },
      "source": [
        "# 모델 테스트 \n",
        "# hidden_size = 100\n",
        "# batch_size = 1\n",
        "# num_layers = 1  # ◆ 위에서 1로 선언했었는데?\n",
        "# embedding_size = 70\n",
        "\n",
        "inp = char_tensor(\"A\")  # 텐서로 변환\n",
        "print(inp)\n",
        "hidden = model.init_hidden()\n",
        "print(hidden.size())\n",
        "out,hidden = model(inp,hidden)\n",
        "print(out.size())  # num_chars =  100 이므로 -> output 크기 = 100"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([36])\n",
            "torch.Size([2, 1, 100])\n",
            "torch.Size([1, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUvu81pgJV1",
        "colab_type": "text"
      },
      "source": [
        "2) 손실 함수와 옵티마이져"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su8Ei5vieod-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29y-4WsCgRvm",
        "colab_type": "text"
      },
      "source": [
        "3) test 함수\n",
        "\n",
        "* 임의의 문자(start_str)로 시작하는 길이 200짜리 모방 글을 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbGMmAgqgQS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    start_str = \"b\"\n",
        "    inp = char_tensor(start_str)\n",
        "    hidden = model.init_hidden()\n",
        "    x = inp\n",
        "\n",
        "    print(start_str,end=\"\")\n",
        "    for i in range(200):\n",
        "        output,hidden = model(x,hidden)\n",
        "\n",
        "        # 여기서 max값을 사용하지 않고 multinomial을 사용하는 이유는 \n",
        "        # 만약 max 값만 쓰는 경우에 생성되는 텍스트가 다 the the the 이런식으로 나오기 때문\n",
        "        # multinomial 함수를 통해 높은 값을 가지는 문자들중에 램덤하게 다음 글자를 뽑아내는 방식으로 \n",
        "        # 자연스러운 텍스트를 생성해보자\n",
        "        output_dist = output.data.view(-1).div(0.8).exp()\n",
        "        #  ★ 0.8 -> 증폭? exp() ?\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = all_characters[top_i]\n",
        "\n",
        "        print(predicted_char,end=\"\")\n",
        "\n",
        "        x = char_tensor(predicted_char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ke4tvrwhV0u",
        "colab_type": "text"
      },
      "source": [
        "1-7. 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSc7ZsqEg-JH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d40d9c99-b47e-4c22-c7d2-9b3af701e30c"
      },
      "source": [
        "for i in range(num_epochs):\n",
        "    # 랜덤한 텍스트 덩어리를 샘플링하고 이를 인덱스 텐서로 변환합니다. \n",
        "    inp,label = random_training_set()\n",
        "    hidden = model.init_hidden()\n",
        "\n",
        "    loss = torch.tensor([0]).type(torch.FloatTensor)\n",
        "    optimizer.zero_grad()\n",
        "    for j in range(chunk_len-1):\n",
        "        x  = inp[j]\n",
        "        y_ = label[j].unsqueeze(0).type(torch.LongTensor)\n",
        "        y,hidden = model(x,hidden)\n",
        "        loss += loss_func(y,y_)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"\\n\",loss/chunk_len,\"\\n\")\n",
        "        test()\n",
        "        print(\"\\n\",\"=\"*100)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " tensor([4.6268], grad_fn=<DivBackward0>) \n",
            "\n",
            "bMo1FG*.^}(7OUp$aJ]]R\n",
            "C\tq_7\"pvurJ@aYO '\r=k<o]h\"Y\rA<<)TE.V\\$/bA!CAUipM@m/(Qcy ;$8X]pB|oA^:8;yl8a;_M\rTMii\\n\"bp\n",
            "D}4i#Au\f\fmDn\ttZi1r9iqbPfmZC3wn%.Te+\\C6gVaKf,f1NEBJ&rtY2\r#\fXb MeK70*nzKdmNh=$DF0O45ggnQ\r~9~|0\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.5591], grad_fn=<DivBackward0>) \n",
            "\n",
            "blis, is,or thit\n",
            " coul,\n",
            "Eon hanruw sor stwWist ceive I sminr nos sony sanesd yat.\n",
            "Wh?t\n",
            "Why goreturr thr, bow:\n",
            "Od mas ony rurin he st ny lD bars no nis il houg bhy aner lifasoO matheves woret fcathe, Bn\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.3313], grad_fn=<DivBackward0>) \n",
            "\n",
            "besy ind:\n",
            "Ang yarthale, blall beve be thin dilch sorn,\n",
            "The I ont torullaf iy?\n",
            "Mensom kopCe urdond thind:\n",
            "Thend were deeith the ter!\n",
            "And or te d, will weas, balad:\n",
            "Houf?\n",
            "\n",
            "ORAUUR:\n",
            "Av! I GF Iurce ht your \n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.2919], grad_fn=<DivBackward0>) \n",
            "\n",
            "bnis Roghise peore bat ay be\n",
            "Rroord say, by ut; in, thy michirs youn the Hantlink and thay nold not thes this'dis mocer; saem than my rath seer that rornou whanm trithe arnidd wurd masmis frarsy I qed \n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.2789], grad_fn=<DivBackward0>) \n",
            "\n",
            "brow in nour, tholle not the; hat, hat lore, whune pass hands, he her.\n",
            "Adland when you heace;\n",
            "The hit have for he mos bave he cringly to meny mare we sill.\n",
            "\n",
            "IO:\n",
            "Hon in the kfurtrett twour hen, not me s\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0553], grad_fn=<DivBackward0>) \n",
            "\n",
            "bent\n",
            "Is hame awach the here,\n",
            "Thay, your by:\n",
            "Whos the deldering homat fure or frad, the caond:\n",
            "Thou whecknd:\n",
            "No mere's alt farair,\n",
            "low shall heet the mos of of cands, brompor your seil will thy wo ther \n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.1515], grad_fn=<DivBackward0>) \n",
            "\n",
            "bull the,\n",
            "Chat the ress\n",
            "Whith to me, as with prown of the goars have deistreds. I with of ip my beter yar you thy the put thou know the fors reman ward ere the cove, he ar sum4 me the wain colloth,\n",
            "Cen\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0475], grad_fn=<DivBackward0>) \n",
            "\n",
            "bree and ence wey sas the sick staes oud make, and thou it ay hear not to say man love tirouning me ard the shuther:\n",
            "To have to nester.\n",
            "\n",
            "BENCINILANS:\n",
            "The me showler the my to low tatties the oure.\n",
            "\n",
            "IUC\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.9842], grad_fn=<DivBackward0>) \n",
            "\n",
            "brone.\n",
            "Bhegefore my beden and exploned ale in grool a wiss well be the with the and what dobly deadrat:\n",
            "So king,\n",
            "I aster?\n",
            "\n",
            "PRICHIO:\n",
            "Yor wize the the\n",
            "ent.\n",
            "\n",
            "GRIEN:\n",
            "Oar'ssing on shone of tahere then we ma\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.9430], grad_fn=<DivBackward0>) \n",
            "\n",
            "breath, bulies mand Dother:\n",
            "I, and the sthe it love prove shold.\n",
            "\n",
            "GRLETS\n",
            "Noth not bether in as surless for poinc, and you shed steming my drow ther. Hasure on the grow and am my a pade and mannows\n",
            "Trev\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0439], grad_fn=<DivBackward0>) \n",
            "\n",
            "bather, Creperous coffeen me that fallands we how work! I with rest him pursen way, and my fall way he that up good is hast now for gone.\n",
            "\n",
            "ASTES:\n",
            "Forn for scay my dops con and that I what whim wriked o\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0335], grad_fn=<DivBackward0>) \n",
            "\n",
            "but would\n",
            "The relaed love and awe.\n",
            "\n",
            "ELIO:\n",
            "Your been would so his courws me, broundest sir stall: be be\n",
            "mis in I voulone and for lemoneforas\n",
            "Of of here ond,\n",
            "Then at\n",
            "in your a day, as a wear not a lanse.\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.1778], grad_fn=<DivBackward0>) \n",
            "\n",
            "be's my be not but be you go that? you me part'd brous have horve you. Ane in knome covee us fords the doth, on fortend of me up shen\n",
            "We the is the gorde seve worsmeet.\n",
            "\n",
            "PRARTIUS:\n",
            "It gows lear soldon w\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([2.0277], grad_fn=<DivBackward0>) \n",
            "\n",
            "ble this not not of dot this of this spillse him that strange mall do them that the for have grainss will be\n",
            "thee.\n",
            "\n",
            "BARIONE:\n",
            "But my what not he come of for deempless?\n",
            "\n",
            "BALICHAPULET:\n",
            "Go,\n",
            "What wordnest r\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8112], grad_fn=<DivBackward0>) \n",
            "\n",
            "bance and in shroved\n",
            "Thy sa lains, bull monet with rowneds.\n",
            "\n",
            "ALUSINTEN:\n",
            "Whin toss.\n",
            "That blord he'll man\n",
            "Bellone als thou arpoly thy have you of near my meely now in you, hery in ruouse to and of this p\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8423], grad_fn=<DivBackward0>) \n",
            "\n",
            "ble ouch\n",
            "is livere.\n",
            "\n",
            "KING RICENTIO:\n",
            "The gad, and of the is so sepes sparso, at the p you where as of a read the parete, my of abpeicilel\n",
            "And pood a foir 'tastre tather, my love is bread it nect a soe h\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.9834], grad_fn=<DivBackward0>) \n",
            "\n",
            "blick you blew, and rew'st miseights and day is brother, bid in the beouted have our the dreak, thou somust thou hake know deak betered, and\n",
            "And it hese contangel,\n",
            "Will thy but, ventless him my been.\n",
            "\n",
            "\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.9158], grad_fn=<DivBackward0>) \n",
            "\n",
            "bate your no This diding ove it here I was distles vewser, of some his abment\n",
            "-you some with ut a conse!\n",
            "\n",
            "BRUEES:\n",
            "My more with the dits of you as noblurus serame.\n",
            "\n",
            "MECENTEL:\n",
            "Adown beteranns your your f\n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.5200], grad_fn=<DivBackward0>) \n",
            "\n",
            "broke, he man teets, your how me are from man and starness you hith you liz?\n",
            "My ligh enpean!\n",
            "\n",
            "JULIEF:\n",
            "But hends are near twand no mor ever\n",
            "To coupt, our now of lifeind conter they from man all the son \n",
            " ====================================================================================================\n",
            "\n",
            " tensor([1.8115], grad_fn=<DivBackward0>) \n",
            "\n",
            "beez them whese anteress CEtide in and he parting suntern of in the farroh thou thee; the burse betrest care you leven the fead could would the treen how love not by dig do, your heart the be your lot-\n",
            " ====================================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOaCO9ohkMvt",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "* GRU 는 다음과 같이 위의 코드에서 RNN 부분을 GRU 로 변경\n",
        "\n",
        "```\n",
        "self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "\n",
        "self.rnn = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers)\n",
        "```\n",
        "\n",
        "* LSTM 은 위의 코드에서 `cell`를 추가\n",
        "![차이점](https://drive.google.com/uc?id=1zYKITpgZa3tCLbTQvV2qBsbt2lOsfFgy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlWraxPche_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xf4oXy-kMWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}